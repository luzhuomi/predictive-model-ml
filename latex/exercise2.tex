\documentclass[10pt]{article}

\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{code}
\usepackage{graphicx}

\usepackage{listings}



\usepackage[english]{babel}
%\usepackage{blindtext}
%\usepackage{fontspec}
%\setmainfont{Arial}

%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{blindtext}
\pagenumbering{arabic}  % Arabic page numbers GM July 2000

\usepackage{authoraftertitle}
\usepackage{fancyhdr}
% Clear the header and footer
\fancyhead{}
\fancyfoot{}
%\lhead{\includegraphics[height=0.7cm]{../logo/nyp-logo-int.png} }
\rhead{\scriptsize \MyTitle}
%\lfoot{\scriptsize Specialist Diploma in Business \& Big Data Analytics
%  \\ Copyright
 % \copyright\ Nanyang Polytechnic. All Rights Reserved.}
\rfoot{\thepage}
\pagestyle{fancy}


\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{0.02}
\renewcommand{\floatpagefraction}{0.95}


\bibliographystyle{plain}

%%\textwidth      150mm \textheight     210mm \oddsidemargin  -2mm \evensidemargin -2mm

\renewcommand{\baselinestretch}{0.987}

\setlength{\parskip}{0.0in}

%\input{macros-ms}

\newcommand{\Nturns}{\, \vdash_{\mbox{\scriptsize lnf}} \,}

\newcommand{\tr}[1]{}
%%\newcommand{\tr}[1]{#1}
%%\newcommand{\nottr}[1]{#1}
\newcommand{\nottr}[1]{}

%\newcommand{\implies}{\supset}
\newcommand{\clabel}[1]{\mbox{(#1)}}
\newcommand{\rat}[1]{\rightarrowtail_{#1}}
\newcommand{\arr}{\rightarrow}
\newcommand{\arrow}{\rightarrow}
\newcommand{\Arr}{\Rightarrow}
\newcommand{\atsign}{@}
\newcommand{\simparrow}[0]{\Longleftrightarrow}
\newcommand{\proparrow}[0]{\Longrightarrow}
\newcommand{\comment}[1]{}
\newcommand{\ignore}[1]{}
%\newcommand{\kl}[1]{{\bf KL:#1}} %%%{\marginpar{\sc kl}{\bf #1}}
\newcommand{\kl}[1]{}
\newcommand{\ms}[1]{{\bf MS:#1}}       %%{\marginpar{\sc ms}{\bf #1}}
%%\newcommand{\jw}[1]{\marginpar{\sc jw}{\bf #1}}
\newcommand{\pjs}[1]{}
%%\newcommand{\ms}[1]{}
\newcommand{\jw}[1]{}
%%\newcommand{\kl}[1]{}


\newcommand{\pow}{\^{}}
\newcommand{\venv}{\Delta}
\newcommand{\mleq}{\mbox{\tt leq}}
\newcommand{\mmleq}{\leq}
\newcommand{\mas}{\mbox{\tt as}}
\newcommand{\mfix}{\mu}

\newcommand{\mysection}[1]{\vspace*{-2mm}\section{#1}\vspace*{-1mm}}
\newcommand{\mysubsection}[1]{\vspace*{-1mm}\subsection{#1}\vspace*{-1mm}}

\newcounter{cnt}
\newtheorem{ex}{Example}
%\newenvironment{example}{
%        \begin{ex}\rm}%
%    {\hfill$\Box$\end{ex}}

\newenvironment{nexample}{
        \begin{ex}\rm}%
        {\end{ex}}

%%\newtheorem{rem}{Remark}
%%\renewenvironment{remark}{
%%        \begin{rem}\rm}%
%%    {\hfill$\Box$\end{rem}}

%%\newenvironment{myexample}{
%%        \begin{ex}\rm}%
%%  {\hfill $\Diamond$\end{ex}}
%\newenvironment{example}{
%        \begin{example}\rm}%
%    {\end{example}}

\newenvironment{ttline}{\begin{trivlist}\item \tt}{\end{trivlist}}
\newenvironment{ttprog}{\begin{trivlist}\small\item \tt
        \begin{tabbing}}{\end{tabbing}\end{trivlist}}


\newcommand{\figcode}[1]
        {\begin{figure*}[t]#1
        \end{figure*}}


\title{Introduction to Spark Machine Learning}
%\author{Kenny Zhuo Ming Lu\\
%  \multicolumn{1}{p{.7\textwidth}}{\centering
%  \emph{School of Information Technology \\ Nanyang Polytechnic \\
%    180 Ang Mo Kio Avenue 8, Singapore 569830}}
%}


\begin{document}
\maketitle \makeatactive
\thispagestyle{fancy}

%\lstset{language=scala}

%\begin{abstract}
%\end{abstract}



\section{Exercises }

\begin{enumerate}
\item check out the source code.
\begin{code}
$ cd examples/spark-ml-examples
\end{code}
%$
\end{enumerate}

\subsection{Tweet classifier}

The school have collected some twitter data on the keyword ``jae''. 
However the term ``jae'' could refer to some 
Korean celebrities having ``jae'' as part of their names, or to joint
admission exercise in Singapore. 
The task here is to build a classifier to differentiate the KPOP tweets
mentioning ``jae'' as a person's name or otherwise.

For example, the following tweet message falls into the category of
Korean Pop because it seems talking about a person's name 
\begin{code}
crazy cool   jae s lee's pic of street singer reflected in raindrops
 tuesday on 2nd ave  
\end{code}
%
On the other hand, the following tweet is not revelant to KPOP. 
\begin{code}
accident closes jae valley rd drivers advised to avoid area seek 
alternate routes
\end{code}
%
To achieve the goal, we need to develop a classifier, which is a
supervised machine learning technique. In this example, we consider
using Support Vector Machine (SVM) as the classifier
algorithm. On the higher level, we need to ``train'' the model with
some manually labelled data and perform some tests against the trained
model. As part of the input requirement the SVM expect the input data
to represented as a label (either yes or no, 1 or 0) accompanied by
the feature vector. The feature vector is a vector of values which
uniquely differentiate one entry from another ideally.  In the machine
learning context, features have to be fixed by the programmers. 

For the ease of illustration, we use a vector of hash codes derived
from the words appearing in the tweet.  Consider the following source
code in \\ {\tt src/main/scala/TweetSVMFilter.scala} 

\begin{code}
object TweetSVMFilter {

  val vector_fixed_size = 30 
  // fixed the size of each vector. 
  // if vectors have different sizes, the gradient descent algorithm will fail
  // cut off if it exceeds, pad zeros if it has less than 30 elements

  def hash(str:String):Int = str.toList.foldLeft(2147483647)((h,c) => 31*h + c)
  
  def to_words(tweet:String):List[String] = tweet.split(" ").toList

  def pad_cap(xs:List[Double],size:Int):List[Double] = xs match 
  {
    case Nil if size > 0 => List.fill(size)(0.0) // fill the rest with 0.0
    case Nil             => Nil 
    case (y::ys) if size == 0 => Nil
    case (y::ys)              => y::(pad_cap(ys,size-1))
  }

  def main(args: Array[String]) {
     
    val sc = new SparkContext("local", "shell")
    // Load training data in LIBSVM format.
    val posTXT:RDD[String] = sc.textFile("data/tweet/label_data/Kpop/*.txt") // .sample(false,0.1)
    val negTXT:RDD[String] = sc.textFile("data/tweet/label_data/othertweet/*.txt") // .sample(false,0.1)
    
    // convert the training data to labeled points
    val posLP:RDD[LabeledPoint] = posTXT.map( (twt:String) => 
    {
      val ws = to_words(twt).map(w => hash(w).toDouble)
      LabeledPoint(1.0, Vectors.dense(pad_cap(ws ,vector_fixed_size).toArray))
    })
    val negLP:RDD[LabeledPoint] = negTXT.map( (twt:String) => 
    { 
      val ws = to_words(twt).map(w => hash(w).toDouble)
      LabeledPoint(0.0, Vectors.dense(pad_cap(ws ,vector_fixed_size).toArray))
    })
    val data = negLP ++ posLP

    // Split data into training (60%) and test (40%).
    val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    val training = splits(0).cache()
    val test = splits(1)

    // Run training algorithm to build the model
    val numIterations = 1
    val model = SVMWithSGD.train(training, numIterations)

    // Clear the default threshold.
    model.clearThreshold()

    // Compute raw scores on the test set. 
    val scoreAndLabels = test.map { point =>
      val score = model.predict(point.features)
      (score, point.label)
    }

    // Get evaluation metrics.
    val metrics = new BinaryClassificationMetrics(scoreAndLabels)
    val auROC = metrics.areaUnderROC()

    println("Area under ROC = " + auROC)	
    sc.stop
  }
}
\end{code}
The {\tt hash} function hashes a string value into an integer.
The {\tt to\_words} function turns a sentence into words.
The {\tt pad\_cap} function takes a list of double values as input {\tt xs} and
a size {\tt size}. If the size of the input
list exceeds the given size, it truncates the input list down to the size.
If the size of the input falls below {\tt size}, it pads the remaining spaces with zeros.
This is to ensure all vectors to the SVM have the same size.
In the {\tt main} function, we first load the labeled data from the
HDFS. We construct the labeled points from the input data. Note that
the vector in a labeled point is constructed from the input text by
taking the first 30 words from the text and converting each word into
a hash code. In the following steps, we combine the positive and the negative labeled
points. By randomly split the labeled points into two sets, namely 60\% for
the model construction and 40\% for the testing.

\begin{enumerate}

\item To compile
\begin{code}
$ sbt package
\end{code}
%$


\item To execute 
\begin{code}
$ spark-submit --class TweetSVMFilter\ 
target/scala-2.11/spark-ml-examples_2.11-0.1.jar
\end{code}
%$
We will observe the output
\begin{code}
Area under ROC = 0.5517569981668705
\end{code}

\end{enumerate}

\subsection{Exercises - improving the results}

Our sample model is very naive and the results as ROC = 0.55 is not
good at all. To understand
why it is the case, we have to think about how SVM works. SVM expects
each training sample is a vector. It plots all the postive and
negative training samples in the vector space and try to derive a
polynomal border to segment the positive and the negatives.  However
the way we built the vectors from the tweets in the earlier section is
volatile under the orders of words and easily affected by noise. For
instance, 
``I love yoo jae suk'' and  ``yoo jae suk is in Singapore'' are two
possible KPOP related tweets. However, they result in two different
vectors using the {\tt hash} function. 
\begin{code}

scala> "I love yoo jae suk".split(" ").toList.map(hash)
res1: List[Int] = List(-2147483606, -2145079311, -2147393606, -2147408465, -2147399190)
\end{code}
and 
\begin{code}

scala> "yoo jae suk is in Singapore".split(" ").toList.map(hash)
res2: List[Int] = List(-2147393606, -2147408465, -2147399190,
-2147481239, -2147481244, -1451355675)
\end{code}



The common values {\tt -2147393606, -2147408465, -2147399190} are
found in different positions within the two vectors. As a result they are in different
dimension of the vector space. It is harder to derive a good
polynomial border that seperate these two vectors with the non-Kpop
training sample. It would be a lot easier if these two vectors are
``nearer'' to each other.

It seems to be making sense to ``normalize'' the vector dimensions. One
possibility to sort all the words arising in the training
data. However this approach is not practical because that would cause
an explosion in size in terms of the vector space. 

In the previous assignment we learned that tf-idf is a good way to find out the
``important'' terms appearing on the corpus. Let's incorporate that
into our Spark machine learning model. For instance, we can run
tf-idf, sort the terms according ot tf-idf score in descending order, 
and collect the top 150 terms, and use them as the vector
dimensions. (This implies that we will have a 150-dimension space.)


\begin{code}
// scala 
import org.apache.spark.rdd._
def to_words(tweet:String):List[String] = 
    remove_stop_words(remove_punct(tweet).split(" ").toList)

val hdfs_nn = "127.0.0.1"
val posTXT:RDD[String] = sc
    .textFile(s"hdfs://${hdfs_nn}:9000/data/tweet/label_data/Kpop/*.txt")
val negTXT:RDD[String] = sc
    .textFile(s"hdfs://${hdfs_nn}:9000/data/tweet/label_data/othertweet/*.txt")

val posTerms:RDD[Seq[String]] = posTXT.map(line => to_words(line).toSeq)
val negTerms:RDD[Seq[String]] = negTXT.map(line => to_words(line).toSeq)
val terms:RDD[Seq[String]] = posTerms ++ negTerms

val tf = terms.flatMap( seq => seq.map((w:String) => (w,1))).reduceByKey(_ + _)
val df = terms.flatMap( seq => seq.toSet.map((w:String) => (w,1))).reduceByKey(_ + _)
val dCount = terms.count()
val tfidf:RDD[(String,Double)] = tf.zip(df)
   .filter( p => p._1._1 == p._2._1 )
   .map(p => (p._1._1, p._1._2 * Math.log(dCount/p._2._2)))
   .sortBy( p => - p._2).cache()
val top:Array[String] = tfidf.take(150).map( p => p._1 )

\end{code}


If we use pyspark or spark-shell to observe {\tt top}, we find 
\begin{code}
//scala 
List("millz", "with", "lee", "suk", "new", "yoo", ...

\end{code}
%

We can create a standardized vector dimension for each tweet. For
instance, the tweet ``I love yoo jae suk'' yields a vector 
\begin{code}
// scala
List(0, 0, 0, 1.0, 0, 1.0, ...)
\end{code}
and  ``yoo jae suk is in Singapore''
\begin{code}
// scala
List(0, 0, 0, 1.0, 0, 1.0, ...)
\end{code}
%
The first elements in both vectors are {\tt 0} because the top TF-IDF word ``
  millz'' is not mentioned in neither tweets. Same observation
applies to the second top word ``with''. The word ``lee'' is present
in both tweets hence the third elements are {\tt 1.0} and {\tt
  1.0}. ``new'' is absent hence the fourth elements are {\tt
  0.0}. ``suk'' is present hence the fifth elements are {\tt 1.0}. As
a result, both tweets will be very ``near'' to each other in the
vector space.

Using this normalize method, we can create an SVM model that will
bring the accuracy (area under ROC) up to 0.70. With a careful
selection of stop words filter, we can achieve 0.78 easily.

The second issue is that the selection of the top 150 TF-IDF terms are
pretty restrictive. It would be good to allow us search for
synonyms of the top 150 terms and give a score in between
{\tt -1.0} and {\tt 1.0}, where {\tt -1.0} means totally opposit and
{\tt 1.0} means exact in meaning. To achieve that we can leverage on
another machine learning model called ``word2vec''. Word2vec is a 2
layer neural network that derived the connection among any two words
appearing in a training corpus. In this exercise, we can use the
word2vec shipped with spark.

\begin{code}
// Scala 
scala> import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
scala> val w2vModel = (new Word2Vec()).fit(terms)
scala> w2vModel.findSynonyms("suk",10).toList
res3: List[(String, Double)] = List((te,0.9078602194786072), 
(hoon,0.9073134660720825), (o,0.9002995491027832), 
(era,0.8942645192146301), (law,0.8914737701416016), 
(yeong,0.8779767751693726), (changmin,0.87352454662323), 
(von,0.872575581073761), (hee,0.8698636293411255), 
(mc,0.8682648539543152))
\end{code}

Recall that {\tt terms} are all the terms we derive from the positive and
negative training data in the last step.
It will give us the top N synonyms found in the corpus. Now the idea
is that if a tweet contains the word ``su'', it will render a vector

\begin{code}
// Scala
List(0, 0, 0, 0.90996772050857544, 0, 0, ...)
\end{code}
because ``su'' and ``suk'' are synonyms based on the training terms.

Now if we incorporate it into our model, we will achieve close to 0.9
accurracy. 

To save our model for future usage without re-training and load it up
in a different application (say, the production application), we can 
\begin{code}
// Scala 
scala> val path = "/my/path/"
scala> svm.save(sc, path)

scala> val sameModel = SVMModel.load(sc, path)
scala> sameModel.predict(Vectors.sparse(2, Seq((0, 1.0), ...)))
\end{code}
%
where the sparse vector is the vector representing the new data sample
that you would like to classify. For instance, we would like to
classify ``apple launch a new iphone'' as a tweet. It needs to be
converted to a vector w.r.t. the TF-IDF top 150 terms and the word2vec
synonyms before the model making a prediction.
\end{document}


\end{document}
